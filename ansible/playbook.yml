# IMPORTANT: This playbook assumes an Ansible inventory setup where:
# 1. The master node is part of a group named 'masters'.
# 2. The master node's inventory_hostname is 'k8s-master' (or resolvable as such for fact delegation).
# 3. Worker nodes are not named 'k8s-master'.
# Example inventory snippet:
# [masters]
# k8s-master ansible_host=1.2.3.4
#
# [workers]
# k8s-worker-1 ansible_host=1.2.3.5
# k8s-worker-2 ansible_host=1.2.3.6
#
# [all:children]
# masters
# workers
- hosts: all
  become: yes
  gather_facts: no # Changed to no, will rely on manual fact gathering in pre_tasks

  pre_tasks:
    - name: Bootstrap Python 3.8 using raw command
      raw: |
        amazon-linux-extras enable python3.8
        yum clean metadata
        yum install -y python3.8
        ln -sf /usr/bin/python3.8 /usr/bin/python3
      register: python_install
      changed_when: true

    - name: Set Python 3.8 as the interpreter
      set_fact:
        ansible_python_interpreter: /usr/bin/python3.8

    - name: Manually gather facts using new interpreter
      setup:

  tasks:
    - name: Set up Kubernetes cluster on Amazon Linux 2
  hosts: all
  become: yes
  vars:
    k8s_version: "1.29"

  tasks:
    - name: Install yum-utils
      shell: yum install -y yum-utils
      when: ansible_distribution == "Amazon"

    - name: Install containerd
      shell: yum install -y containerd
      register: containerd_result
      retries: 3
      delay: 10
      until: containerd_result.rc == 0
      when: ansible_distribution == "Amazon"

    - name: Ensure containerd service exists
      stat:
        path: /usr/lib/systemd/system/containerd.service
      register: containerd_service_file

    - name: Reload systemd if containerd unit exists
      shell: systemctl daemon-reexec
      when: containerd_service_file.stat.exists

    - name: Enable and start containerd
      shell: systemctl enable --now containerd
      when: containerd_service_file.stat.exists

    - name: Load br_netfilter module
      shell: modprobe br_netfilter

    - name: Set sysctl for Kubernetes networking
      sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: 1
        state: present
        reload: yes

    - name: Add Kubernetes repo
      shell: |
        cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
        [kubernetes]
        name=Kubernetes
        baseurl=https://pkgs.k8s.io/core:/stable:/v{{ k8s_version }}/rpm/
        enabled=1
        gpgcheck=1
        repo_gpgcheck=1
        gpgkey=https://pkgs.k8s.io/core:/stable:/v{{ k8s_version }}/rpm/repodata/repomd.xml.key
        EOF
      when: ansible_distribution == "Amazon"

    - name: Ensure kubelet systemd drop-in directory exists
      file:
        path: /etc/systemd/system/kubelet.service.d
        state: directory
        mode: '0755'

    - name: Create kubelet containerd config drop-in
      copy:
        dest: /etc/systemd/system/kubelet.service.d/0-containerd.conf
        content: |
          [Service]
          Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock"

    - name: Reload systemd after drop-in
      shell: systemctl daemon-reexec

    - name: Install kubelet, kubeadm, and kubectl
      shell: yum install -y kubelet kubeadm kubectl
      register: kube_result
      retries: 5
      delay: 10
      until: kube_result.rc == 0
      when: ansible_distribution == "Amazon"

    - name: Enable kubelet
      shell: systemctl enable --now kubelet

    - name: Restart kubelet to apply config
      shell: systemctl restart kubelet

    - name: Enable IP forwarding
      sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        state: present
        reload: yes

    # ==============================================================================
    # MASTER NODE CONFIGURATION
    # ==============================================================================
    # Note: In a production setup, use proper Ansible inventory groups instead of inventory_hostname checks.
    - name: Master Node Setup Block
      block:
        - name: Initialize Kubernetes cluster (master node)
          shell: kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=NumCPU,Mem # Added ignore for potential lab resource constraints
          args:
            creates: /etc/kubernetes/admin.conf
          register: kubeadm_init_result

        - name: Display kubeadm init output
          debug:
            var: kubeadm_init_result.stdout_lines

        - name: Create .kube directory for ec2-user
          file:
            path: /home/ec2-user/.kube
            state: directory
            owner: ec2-user
            group: ec2-user
            mode: '0755'

        - name: Copy admin.conf to ec2-user's .kube directory
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /home/ec2-user/.kube/config
            remote_src: yes
            owner: ec2-user
            group: ec2-user
            mode: '0600'

        - name: Deploy Flannel CNI plugin
          shell: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          when: inventory_hostname == 'k8s-master'
    
        - name: Join worker nodes to the cluster
          shell: kubeadm join 172.31.88.98:6443 --token cj86wm.x8asjft6rt2fq0ok \
                 --discovery-token-ca-cert-hash sha256:8c180cf118d70ab7ecab737dca9e9cd7984c0b8746350210d267e98460c475dc
          when: inventory_hostname != 'k8s-master'"

    # ==============================================================================
    # DEPLOY KUBERNETES MANIFESTS (ON MASTER NODE)
    # ==============================================================================
    - name: Deploy Kubernetes Manifests Block
      block:
        - name: Create directory for kubernetes manifests on EC2 (master)
          file:
            path: /home/ec2-user/kubernetes_manifests
            state: directory
            owner: ec2-user
            group: ec2-user
            mode: '0755'

        - name: Copy Kubernetes manifest files to EC2 (master)
          copy:
            src: ../kubernetes/ # Assuming this path is relative to the playbook file
            dest: /home/ec2-user/kubernetes_manifests/
            owner: ec2-user
            group: ec2-user
            mode: '0644'

        - name: Create DockerHub imagePullSecret for Kubernetes (master)
          shell: |
            kubectl create secret docker-registry dockerhub-secret \
              --docker-username="{{ dockerhub_username }}" \
              --docker-password="{{ dockerhub_password }}" \
              --docker-email="{{ dockerhub_email }}" \
              --dry-run=client -o yaml | kubectl apply -f -
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          vars:
            dockerhub_username: "{{ lookup('env', 'DOCKERHUB_USERNAME') }}"
            dockerhub_password: "{{ lookup('env', 'DOCKERHUB_PASSWORD') }}"
            dockerhub_email: "{{ lookup('env', 'DOCKERHUB_EMAIL') }}"
          become_user: ec2-user # Run kubectl as ec2-user
          # Consider adding a check if the secret already exists

        - name: Apply Selenium Hub Service (master)
          shell: kubectl apply -f /home/ec2-user/kubernetes_manifests/selenium-hub-service.yml
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          become_user: ec2-user

        - name: Apply BDD Service Service (master)
          shell: kubectl apply -f /home/ec2-user/kubernetes_manifests/bdd-service-service.yml
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          become_user: ec2-user

        - name: Apply Selenium Hub Deployment (master)
          shell: kubectl apply -f /home/ec2-user/kubernetes_manifests/selenium-hub-deployment.yml
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          become_user: ec2-user

        - name: Apply Selenium Node Chrome Deployment (master)
          shell: kubectl apply -f /home/ec2-user/kubernetes_manifests/selenium-node-chrome-deployment.yml
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          become_user: ec2-user

        - name: Apply BDD Service Deployment (master)
          shell: kubectl apply -f /home/ec2-user/kubernetes_manifests/bdd-service-deployment.yml
          environment:
            KUBECONFIG: /home/ec2-user/.kube/config
          become_user: ec2-user

      when: inventory_hostname == "k8s-master" # Replace with group targeting (e.g., when: "'masters' in group_names")

  handlers:
    - name: Restart containerd
      systemd:
        name: containerd
        state: restarted

    - name: Configure sysctl for Kubernetes
      command: sysctl --system




